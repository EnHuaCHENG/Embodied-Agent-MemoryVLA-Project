# MemoryVLA: Perceptual-Cognitive Memory in Vision-Language-Action Models for Robotic Manipulation
This is the code for the paper "MemoryVLA: Perceptual-Cognitive Memory in Vision-Language-Action Models for Robotic Manipulation".

### üè†[Project Page](https://shihao1895.github.io/MemoryVLA/) | üìë[Paper](https://arxiv.org/abs/2508.19236) | ü§ó[Models](https://huggingface.co/shihao1895)

## üåü News

- üî• \[2025-8-26\] Our paper [MemoryVLA](https://arxiv.org/abs/2508.19236) is now on arxiv!

## Overview

MemoryVLA is a Cognition-Memory-Action framework for robotic manipulation inspired by human memory systems. It builds a hippocampal-like perceptual-cognitive memory to capture the temporal dependencies essential for current decision-making, enabling long-horizon, temporally aware action generation.

![MemoryVLA Overview](images/intro.png)

## TODO

We will release 2 versions of the code by the end of October, based on the OpenVLA codebase (MemoryVLA) and our self-developed Dexbotic codebase (MemoryVLA+), the latter having higher simulation performance.

- [ ] MemoryVLA (OpenVLA codebase) Code Release
- [ ] MemoryVLA (OpenVLA codebase) Model Weights Release
- [ ] MemoryVLA+ (Dexbotic codebase) Code Release
- [ ] MemoryVLA+ (Dexbotic codebase) Model Weights Release

## Results on Simulation Benchmark

## Citation
If you find our work helpful in your research, please consider citing:

```bibtex
@article{shi2025memoryvla,
  title={MemoryVLA: Perceptual-Cognitive Memory in Vision-Language-Action Models for Robotic Manipulation},
  author={Shi, Hao and Xie, Bin and Liu, Yingfei and Sun, Lin and Liu, Fengrong and Wang, Tiancai and Zhou, Erjin and Fan, Haoqiang and Zhang, Xiangyu and Huang, Gao},
  journal={arXiv preprint arXiv:2508.19236},
  year={2025}
}
```
